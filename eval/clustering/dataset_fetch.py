"""

Fetches the abstracts given the PMIDs generated by `dataset_pmids.py`

"""

import argparse
import requests
import xml.etree.ElementTree as ET
import json
from tqdm import tqdm
import time
import traceback


def _efetch(pmids):
    for i in range(3):
        try:
            res = requests.post("https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi", data={
                "db": "pubmed",
                "retmode": "xml",
                "rettype": "abstract",
                "id": ",".join(pmids)
            })
            return res.content
        except (requests.exceptions.ConnectionError, requests.exceptions.Timeout, requests.exceptions.ChunkedEncodingError):
            time.sleep(3**(i+1))
            continue
    raise RuntimeError("Cannot retrieve abstracts")


def _getAbstractFromXML(xml):
    def _getInnerText(xml):
        # Gets the content of a tag as text without other inner tags
        return ''.join(xml.itertext())
    
    # Abstract not available, discard article
    if xml.find("MedlineCitation/Article/Abstract") is None: 
        return None

    # Abstract
    abstract_sections = xml.find("MedlineCitation/Article/Abstract").findall("AbstractText")
    abstract = ""
    for section_xml in abstract_sections:
        section_text = _getInnerText(section_xml)
        if "Label" in section_xml.attrib: # Appends section title if set
            section_text = f"{section_xml.attrib['Label']} {section_text}"
        abstract = f"{abstract}\n{section_text}"
    abstract = abstract.strip()

    return abstract


if __name__ == "__main__":
    parser = argparse.ArgumentParser(prog="Fetch abstract from PMIDs")
    parser.add_argument("--pmids", type=str, default="./pmids.json", required=False, help="Path for the pmids file (output of dataset_pmids.py)")
    parser.add_argument("--output", type=str, default="./dataset.json", required=False, help="Path for the output file")
    args = parser.parse_args()

    with open(args.pmids, "r") as f:
        dataset_pmids = json.load(f)

    dataset = []
    batch_size = 500

    try:
        for entry in dataset_pmids:
            entry_abs = []
            batched_pmids = [entry["pmids"][i : i+batch_size] for i in range(0, len(entry["pmids"]), batch_size)]

            # Retrieves abstracts
            for pmids in tqdm(batched_pmids, desc=f"Fetching query \"{entry['query']}\"", leave=False): # type: ignore
                query_xml = _efetch(pmids)
                query_tree = ET.fromstring(query_xml)
                for article_tree in query_tree:
                    abstract = _getAbstractFromXML(article_tree)
                    if abstract is not None:
                        entry_abs.append(abstract)
                time.sleep(3)

            print(f"{entry['query']} -- fetched {len(entry_abs)} abstracts from {len(entry['pmids'])} PMIDs")
            dataset.append({
                "query": entry["query"],
                "abstracts": entry_abs
            })
    except Exception:
        print(traceback.format_exc())
        print("An error occured, saving what I've got")

    with open(args.output, "w", encoding="utf-8") as fout:
        json.dump(dataset, fout, ensure_ascii=False, indent=None)
        